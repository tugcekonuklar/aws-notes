## S3 101

* S3 stands for simple storage service
* it provides the secure, durable and highly scalable object storage.
* it's object storage and it's really for files, images, webpages and that kind of thing.
* it's also super scalable and allows you to store and retrieve any amount of data from anywhere on the web at a very
  low cost.
* S3 is **object based** storage so it manages it data as objects rather than in file systems like an operating system
  or in
  data blocks.
* you can upload any type of file that you can think of to S3.
    * For example, photos, videos, code, documents and text files.
* However, it cannot be used to run an operating system or a database.
* S3 you get unlimited storage.
    * So the total volume of data and the number of objects that you can store is unlimited.
* S3 objects can be up to 5 TB terabytes in size
    * they can range in size from a minimum of 0byte->5TB.
* within S3 the files are stored in **buckets**.
    * And a bucket is similar to a folder and it's really just a container and it's the name that AWS uses for the
      location where you are storing your files.
* S3 actually has a universal namespace.
    * And that means that all AWS accounts share the same S3 namespace and each S3 bucket name must be globally unique.
    * similar to a DNS address or an internet address which must also be globally unique.
* S3 URL:
    * https://**bucket-name**.s3.Region.amazonaws.com/**key-name**
    * https://**tugcecoloudguru**.s3.Region.amazonaws.com/**cat.jpeg**
    * keyname is the name of the object
* when you upload a file into an S3 bucket you are going to receive an HTTP 200 code if the upload was successful.
    * you'll only see this code if you're uploading using the API or the command line interface
* S3 is a key value store
    * the key which is simply the name
      of the object i.e cat.jpg.
    * the value, and this is really the data itself which is made up of a sequence of bytes which makes up the object.
    * a version ID which is important when we come to store multiple versions of the same object.
    * metadata and if you haven't heard that term before metadata just really means data about data
        * For example, content type, last modified or you could even add the name of the team that owns the file or the
          project that the file is related to.
    * ![s3 object](img/s3-101-1.png)
* s3 is a safe place to store your data files and the data is spread across multiple devices and multiple facilities to
  ensure availability and durability.
    * And what that means is that Amazon could lose one of their devices or facilities and the S3 service will still be
      available.
* S3 is designed to be both highly available and highly durable.
    * S3 is built for between 99.95 and 99.99% service availability depending on the S3 tier that you select to store
      your data.
* it is also designed for 11 nines (99,9999999999) durability for data stored in S3.
    * we say durability that is all about your data being stored safely and not getting lost or corrupted
* S3 offers tiered storage.
    * So it offers a range of storage courses or tiers designed for different use cases depending on the type of data
      that you might be storing and your own business requirements.
* S3 has lifecycle management which allows you to set rules and define rules to automatically transition objects to a
  cheaper storage tier or even delete objects which are no longer required after a set period of time.
* versioning.
    * So with versioning all versions of an object are stored and can be retrieved including deleted objects.
    * So that means we can keep multiple historical versions of the same file allowing you to roll back to a previous
      version if you accidentally changed or deleted your file.
* Security:
    * S3 can enable server-side encryption:
        * you can set up default encryption on a bucket which will encrypt all new objects when they are stored in the
          bucket.
    * got access control lists:
        * which allow you to define which AWS accounts or groups are granted access and the type of access they're
          granted as well.
        * And you can attach S3 access control lists to individual objects within a bucket.
        * So that really gives you fine grained access control.
    * bucket policies
        * S3 bucket policies specify which actions are allowed or denied.
        * For example, you can use a bucket policy to allow a user named Alice to put but not delete objects within a
          bucket that you own.

## Reviewing s3 Storage Classes

* S3 Standard:
    * Data stored redundantly across multi devices in multi facilities.
        * Data will store >= 3 availability zones
    * Gets 99,99% availability
    * Gets 99,99999...% 11 9 durability
    * Perfect for frequently accessed data
    * It is designed to be suitable for most workloads,
    * it is the default storage class.
    * use cases include things like websites, content distribution, mobile and gaming applications, and big data
      analytics.
* S3 Standard-Infrequent Access:
    * this is designed for infrequently access data,
        * so data that you store can be accessing few times in a month but not daily bases
    * designed for rapid access
        * it can be use less frequently but requires to access rapid
    * there's a low per gigabyte storage price and a per gigabyte retrieval fee.
    * Use cases include long-term storage, backups, and disaster recovery files.
    * min storage duration is 30 days
    * Gets 99,99% availability
    * Gets 99,99999...% 11 9 durability
    * Data will store >= 3 availability zones
* S3 One zone-Infrequent Access:
    * Same as S3 infrequent Access storage but stored redundantly within a single availability zone instead of being
      stored in multiple availability zones.
    * Cost effective because it will cost less 20% than S3 Infrequent Access
    * It is great long-live infrequently non-critical data
    * Gets 99,95% availability because of one zone availability
    * Gets 99,99999...% 11 9 durability
* S3 Glacier:
    * very cheap and cost effect storage
    * optimised for data that is very infrequently access
    * you pay fee when you access data and only use for archiving
    * retrieval time is between 1 minute to 12 hours
    * great for historical data that you will reach few times in a year
    * min storage 90 days
    * Gets 99,99% availability
    * Gets 99,99999...% 11 9 durability
    * Data will store >= 3 availability zones
* S3 Glacier Deep Archive:
    * archiving rarely access data within 12 hours retrieval time
    * for example financial data that needs to access once or twice in a year or never, but you still need to store them
      because compliance purpose
    * min 180 days storage requires.
    * Gets 99,99% availability
    * Gets 99,99999...% 11 9 durability
    * Data will store >= 3 availability zones
* S3 Intelligent Tier:
    * When we need to access data frequently or non-frequently, we can use
    * has 2 tiers frequently and infrequently
    * with Intelligent-Tiering, S3 will automatically move your data to the most cost-effective tier based on how
      frequently you access each object.
    * This is great for optimizing your costs and they do charge you an additional monthly fee,
        * but it's a really small fee of $0.0025 per 1,000 objects, so the fee is negligible.
    * Gets 99,9% availability
    * Gets 99,99999...% 11 9 durability
    * Data will store >= 3 availability zones
* ![S3 Storage Classes Summary](img/s3-2.png)

## Securing s3 buckets

* Secure by default:
    * all newly created buckets are completely private
    * only the bucket owner can upload/delete etc the files
    * No access on public by default
* Control Access to Bucket
    * Bucket Policy - bucket level:
        * it allows to give permission to pther users for some actions via defining policy
        * you can setup access control your buckets by define bucket policies
        * permissions granted by the policy are going to apply to all of the objects within the bucket.
        * not applying permissions to individual objects
        * you cannot attach a bucket policy to an individual object
        * it's not fine-grained access control
        * they are really useful.
            * If you have a group of files in the same bucket and they need to be accessed by the same people.
        * bucket policies are written in JSON,!
        * [Bucket policy](img/s3-3.png)
    * Bucket Access Control Lists(ACLs) - object level:
        * Access Control Lists are applied at an object level.
            * So this means that we can apply different permissions for different objects within the same bucket.
        * using Access Control Lists, we can grant different types of access to different objects within our bucket.
        * And we can define which accounts and groups are granted access and also the type of access.
            * For example, read, write or full control.
        * it's bucket ACLs which give you the fine-grained access control for objects within your S3 buckets. So you can
          grant a different type of access to different objects within the same bucket.
            * For example, you can apply different permissions for different objects for different users and groups.
* Access Logs:
    * these are not enabled by default but you can go in and enable S3 access logs.
    * this will log all of the requests made to your S3 buckets.
        * For example, every time a user makes a request to upload a file, read a file or delete a file.

* Object Lock:
    * Store objects using an write-once-read-many(Worm) model to prevent object being delete or overriten for a fix
      amount of time.

## S3 Encryption

* Encryption in transit:
    * means encrypting the data when you were sending it to and from your S3 buckets
        * SSL/TLS Transport Layer Security:
            * if you see SSL or TLS, then just know that that is encryption in transit
        * HTTPS:
            * typically that means we are using HTTPS to upload and download files between our local system and S3
* Encryption at rest - server-side encryption:
    * SSE-S3:
        * encrypts your data using encryption keys managed by S3.
        * each object is encrypted using its own unique key.
        * And as an additional step, they also encrypt the key itself with a master key, which they rotate for you.
        * Amazon manages the keys for you, so you don't need to worry about managing your own keys.
        * it uses AES 256-bit encryption, so Advanced Encryption Standard 256-bit.
        * this is very easy to enable
        * you can do that while you are uploading a file and you can also encrypt existing files as well
    * SSE-KMS:
        * this is with the AWS Key Management Service.
        * AWS manage the keys for you, but KMS comes with some additional benefits.
        * So firstly, you get separate permissions for the use of an additional key called an envelope key.
        * And that is a key which actually encrypts your data's encryption key.
        * So it gives you an added level of protection against unauthorized access.
        * And you also get an audit trail, which records the use of your encryption key.
        * And you can see when your key has been used, who used it, and what they did.
    * SSE-C:
        * which uses an encryption key that you provide yourself.
        * And this is where AWS manage the encryption and decryption activities, but you manage your own keys.
        * So that means you are in charge of administering the keys, rotating them, and the lifecycle of the keys as
          well.
* Encryption at rest - Client side Encryption:
    * this is where you encrypt the files yourself before uploading them into S3.

* you may have a requirement to enforce server-side encryption for any files stored in S3. how can you do that?
    * firstly, we can use the console.
        * can select the encryption setting on the S3 bucket
          and that is really the easiest way to do it and it's just a checkbox in the console
    * Bucket Policy:
        * every time a file is uploaded to S3, a PUT request is initiated,
        * the file is going to be transmitted into S3. So in this case, it's going to use HTTP.
        * hen expect 100-continue. This just tells S3 not to send your request body until it receives an
          acknowledgement. So that means that S3 can actually reject your message based on the contents of this header.
        * if the file is to be encrypted at upload time, then a special parameter will be included in the PUT request
          header.
            * ` x-amz-server-side` header parameter, 2 options in here
                * `x-amz-server-side encryption: AES256` : that is with S3 managed keys
                * `x-amz-server-side encryption: aws:kms` : this uses SSE-KMS, which uses KMS managed keys.
        * when this parameter is included in the header of the PUT request, it tells S3 to encrypt the object at the
          time of upload using the specified encryption method.
            * ![Put mrssage](img/s2-enc-1.png)
        * you can create a Bucket Policy which denies any S3 PUT request, which does not include this parameter in the
          request header.
        * this example policy explicitly denies any requests that are not using AWS Secure Transport.
        * So that means with this policy, S3 will only serve content over HTTPS SSL, and it will deny all unencrypted
          HTTP access.
            * ![bucket policy](img/s2-enc-2.png)
* ![Exam tips](img/s2-enc-3.png)

## CORS Configuration
![cors demo](img/cors-1.png)
* We will create 2 s3 buckets and publish as static webside from PRoperties -> Satatic Website section
    * bucket has to be publicly available
    * load index, loadpage and error.html with public Read ACLs permission for both buckets
    * then the second bucket which has loadpage.html needs to allow CORS thats why,
      * we went  to Permission -> CORS enable and wrote that permission
        ![Cors permission](img/cors-2.png)