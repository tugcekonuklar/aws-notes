## S3 101

* S3 stands for simple storage service
* it provides the secure, durable and highly scalable object storage.
* it's object storage and it's really for files, images, webpages and that kind of thing.
* it's also super scalable and allows you to store and retrieve any amount of data from anywhere on the web at a very
  low cost.
* S3 is **object based** storage so it manages it data as objects rather than in file systems like an operating system
  or in
  data blocks.
* you can upload any type of file that you can think of to S3.
    * For example, photos, videos, code, documents and text files.
* However, it cannot be used to run an operating system or a database.
* S3 you get unlimited storage.
    * So the total volume of data and the number of objects that you can store is unlimited.
* S3 objects can be up to 5 TB terabytes in size
    * they can range in size from a minimum of 0byte->5TB.
* within S3 the files are stored in **buckets**.
    * And a bucket is similar to a folder and it's really just a container and it's the name that AWS uses for the
      location where you are storing your files.
* S3 actually has a universal namespace.
    * And that means that all AWS accounts share the same S3 namespace and each S3 bucket name must be globally unique.
    * similar to a DNS address or an internet address which must also be globally unique.
* S3 URL:
    * https://**bucket-name**.s3.Region.amazonaws.com/**key-name**
    * https://**tugcecoloudguru**.s3.Region.amazonaws.com/**cat.jpeg**
    * keyname is the name of the object
* when you upload a file into an S3 bucket you are going to receive an HTTP 200 code if the upload was successful.
    * you'll only see this code if you're uploading using the API or the command line interface
* S3 is a key value store
    * the key which is simply the name
      of the object i.e cat.jpg.
    * the value, and this is really the data itself which is made up of a sequence of bytes which makes up the object.
    * a version ID which is important when we come to store multiple versions of the same object.
    * metadata and if you haven't heard that term before metadata just really means data about data
        * For example, content type, last modified or you could even add the name of the team that owns the file or the
          project that the file is related to.
    * ![s3 object](img/s3-101-1.png)
* s3 is a safe place to store your data files and the data is spread across multiple devices and multiple facilities to
  ensure availability and durability.
    * And what that means is that Amazon could lose one of their devices or facilities and the S3 service will still be
      available.
* S3 is designed to be both highly available and highly durable.
    * S3 is built for between 99.95 and 99.99% service availability depending on the S3 tier that you select to store
      your data.
* it is also designed for 11 nines (99,9999999999) durability for data stored in S3.
    * we say durability that is all about your data being stored safely and not getting lost or corrupted
* S3 offers tiered storage.
    * So it offers a range of storage courses or tiers designed for different use cases depending on the type of data
      that you might be storing and your own business requirements.
* S3 has lifecycle management which allows you to set rules and define rules to automatically transition objects to a
  cheaper storage tier or even delete objects which are no longer required after a set period of time.
* versioning.
    * So with versioning all versions of an object are stored and can be retrieved including deleted objects.
    * So that means we can keep multiple historical versions of the same file allowing you to roll back to a previous
      version if you accidentally changed or deleted your file.
* Security:
    * S3 can enable server-side encryption:
        * you can set up default encryption on a bucket which will encrypt all new objects when they are stored in the
          bucket.
    * got access control lists:
        * which allow you to define which AWS accounts or groups are granted access and the type of access they're
          granted as well.
        * And you can attach S3 access control lists to individual objects within a bucket.
        * So that really gives you fine grained access control.
    * bucket policies
        * S3 bucket policies specify which actions are allowed or denied.
        * For example, you can use a bucket policy to allow a user named Alice to put but not delete objects within a
          bucket that you own.

## Reviewing s3 Storage Classes

* S3 Standard:
    * Data stored redundantly across multi devices in multi facilities.
        * Data will store >= 3 availability zones
    * Gets 99,99% availability
    * Gets 99,99999...% 11 9 durability
    * Perfect for frequently accessed data
    * It is designed to be suitable for most workloads,
    * it is the default storage class.
    * use cases include things like websites, content distribution, mobile and gaming applications, and big data
      analytics.
* S3 Standard-Infrequent Access:
    * this is designed for infrequently access data,
        * so data that you store can be accessing few times in a month but not daily bases
    * designed for rapid access
        * it can be use less frequently but requires to access rapid
    * there's a low per gigabyte storage price and a per gigabyte retrieval fee.
    * Use cases include long-term storage, backups, and disaster recovery files.
    * min storage duration is 30 days
    * Gets 99,99% availability
    * Gets 99,99999...% 11 9 durability
    * Data will store >= 3 availability zones
* S3 One zone-Infrequent Access:
    * Same as S3 infrequent Access storage but stored redundantly within a single availability zone instead of being
      stored in multiple availability zones.
    * Cost effective because it will cost less 20% than S3 Infrequent Access
    * It is great long-live infrequently non-critical data
    * Gets 99,95% availability because of one zone availability
    * Gets 99,99999...% 11 9 durability
* S3 Glacier:
    * very cheap and cost effect storage
    * optimised for data that is very infrequently access
    * you pay fee when you access data and only use for archiving
    * retrieval time is between 1 minute to 12 hours
    * great for historical data that you will reach few times in a year
    * min storage 90 days
    * Gets 99,99% availability
    * Gets 99,99999...% 11 9 durability
    * Data will store >= 3 availability zones
* S3 Glacier Deep Archive:
    * archiving rarely access data within 12 hours retrieval time
    * for example financial data that needs to access once or twice in a year or never, but you still need to store them
      because compliance purpose
    * min 180 days storage requires.
    * Gets 99,99% availability
    * Gets 99,99999...% 11 9 durability
    * Data will store >= 3 availability zones
* S3 Intelligent Tier:
    * When we need to access data frequently or non-frequently, we can use
    * has 2 tiers frequently and infrequently
    * with Intelligent-Tiering, S3 will automatically move your data to the most cost-effective tier based on how
      frequently you access each object.
    * This is great for optimizing your costs and they do charge you an additional monthly fee,
        * but it's a really small fee of $0.0025 per 1,000 objects, so the fee is negligible.
    * Gets 99,9% availability
    * Gets 99,99999...% 11 9 durability
    * Data will store >= 3 availability zones
* ![S3 Storage Classes Summary](img/s3-2.png)

## Securing s3 buckets

* Secure by default:
    * all newly created buckets are completely private
    * only the bucket owner can upload/delete etc the files
    * No access on public by default
* Control Access to Bucket
    * Bucket Policy - bucket level:
        * it allows to give permission to pther users for some actions via defining policy
        * you can setup access control your buckets by define bucket policies
        * permissions granted by the policy are going to apply to all of the objects within the bucket.
        * not applying permissions to individual objects
        * you cannot attach a bucket policy to an individual object
        * it's not fine-grained access control
        * they are really useful.
            * If you have a group of files in the same bucket and they need to be accessed by the same people.
        * bucket policies are written in JSON,!
        * [Bucket policy](img/s3-3.png)
    * Bucket Access Control Lists(ACLs) - object level:
        * Access Control Lists are applied at an object level.
            * So this means that we can apply different permissions for different objects within the same bucket.
        * using Access Control Lists, we can grant different types of access to different objects within our bucket.
        * And we can define which accounts and groups are granted access and also the type of access.
            * For example, read, write or full control.
        * it's bucket ACLs which give you the fine-grained access control for objects within your S3 buckets. So you can
          grant a different type of access to different objects within the same bucket.
            * For example, you can apply different permissions for different objects for different users and groups.
* Access Logs:
    * these are not enabled by default but you can go in and enable S3 access logs.
    * this will log all of the requests made to your S3 buckets.
        * For example, every time a user makes a request to upload a file, read a file or delete a file.

* Object Lock: 
  * Store objects using an write-once-read-many(Worm) model to prevent object being delete or overriten for a fix amount of time.
